# retrain_anids.py
#
# å°è£ A-NIDS çš„ Phase 3 (éŸ¿æ‡‰) é‚è¼¯ã€‚
# (æ–°) æ›´æ–°ï¼šç¾åœ¨æœƒå‹•æ…‹å¹³è¡¡è³‡æ–™ï¼š
# 1. è¼‰å…¥ D_new (100%)
# 2. åˆ†å‰²ç‚º D_new_train (70%) å’Œ D_new_test (30%)
# 3. ç”Ÿæˆ D_old_fakeï¼Œä½¿å…¶ç¸½æ•¸ç­‰æ–¼ D_new_train (70%) çš„ç¸½æ•¸
# 4. åœ¨ (D_old_fake + D_new_train) ä¸Šè¨“ç·´
# 5. åœ¨ D_new_test å’Œ D_old_test ä¸Šè©•ä¼°

import logging
import os
import joblib
import pandas as pd
import torch
from argparse import Namespace # ç”¨æ–¼é¡å‹æç¤º
import math

# å°å…¥ A-NIDS æ¨¡çµ„
from preprocessing import load_and_clean_data
from adaptive_module import check_for_drift
from generation_module import generate_stacked_data
from detect_module import detect_module
from train_and_eval import train_model, evaluate_model, plot_training_history

# (æ–°) å°å…¥ train_test_split
from sklearn.model_selection import train_test_split

from utils import set_seed


def phase_3_retrain(artifacts_dir: str, 
                    data_2018_path: str,
                    X_test_2017_tensor: torch.Tensor, # ç”¨æ–¼æœ€çµ‚è©•ä¼° (D_old 30% æ¸¬è©¦é›†)
                    y_test_2017_tensor: torch.Tensor, # ç”¨æ–¼æœ€çµ‚è©•ä¼° (D_old 30% æ¸¬è©¦é›†)
                    X_train_2018_tensor: torch.Tensor, # ç”¨æ–¼ã€Œåµæ¸¬ã€ (D_new 100% è©•ä¼°è³‡æ–™)
                    y_train_2018_tensor: torch.Tensor, # ç”¨æ–¼ã€Œåµæ¸¬ã€ (D_new 100% è©•ä¼°è³‡æ–™)
                    X_test_2018_tensor: torch.Tensor, # ç”¨æ–¼ã€Œåµæ¸¬ã€ (D_new 100% è©•ä¼°è³‡æ–™)
                    y_test_2018_tensor: torch.Tensor, # ç”¨æ–¼ã€Œåµæ¸¬ã€ (D_new 100% è©•ä¼°è³‡æ–™)
                    input_features: int, 
                    num_classes: int,
                    args: Namespace): # å‚³å…¥ argparse çš„åƒæ•¸
    set_seed(42)
    """
    åŸ·è¡Œ A-NIDS çš„ Phase 3ï¼šåµæ¸¬ã€ç”Ÿæˆã€é‡æ–°è¨“ç·´ã€è©•ä¼°
    """
    logging.info("="*50)
    logging.info("PHASE 3: A-NIDS éŸ¿æ‡‰ (åµæ¸¬ã€ç”Ÿæˆã€é‡æ–°è¨“ç·´)")
    logging.info("="*50)

    # --- 3.1: Adaptive Module åµæ¸¬æ¼‚ç§» ---
    # æˆ‘å€‘ä½¿ç”¨ Phase 2 å·²ç¶“è™•ç†å¥½çš„ X_test_2018_tensor (ä»£è¡¨ D_new çš„ 100%) ä¾†é€²è¡Œåµæ¸¬
    logging.info("--- 3.1: Adaptive Module æ­£åœ¨æª¢æŸ¥è³‡æ–™æ¼‚ç§»... ---")
    try:
        # X_test_2018_tensor æ˜¯ PyTorch Tensor, .numpy() è½‰æ›ç‚º NumPy
        drift_detected = check_for_drift(X_train_2018_tensor.numpy(), artifacts_dir)
    except Exception as e:
        logging.error(f"Adaptive Module æª¢æŸ¥å¤±æ•—: {e}", exc_info=True)
        return

    # if not drift_detected:
    #     logging.info("Adaptive Module æœªåµæ¸¬åˆ°é¡¯è‘—æ¼‚ç§»ã€‚A-NIDS éŸ¿æ‡‰ä¸­æ­¢ã€‚")
    #     return

    # --- (æ–°) æ­¥é©Ÿ 3.2: è¼‰å…¥ D_new_real (2018) ä¸¦ *åˆ†å‰²* ---
    logging.info("--- 3.2: è¼‰å…¥ä¸¦æ¸…ç† D_new (real) 2018 è³‡æ–™... ---")
    # å‡è¨­åœ¨ä¸Šä¸€æ­¥é©Ÿä¸­ï¼Œæ‚¨å·²ç¶“å®Œæˆäº†åˆ†å‰²ä¸¦ç”¢ç”Ÿäº†ä»¥ä¸‹ Tensorï¼š
    # D_new_train_tensor (ç‰¹å¾µ) å’Œ y_new_train_tensor (æ¨™ç±¤)

    # ä½¿ç”¨ y_new_train_tensor çš„é•·åº¦ä½œç‚ºåƒè€ƒç¸½æ•¸
    num_new_train_samples = len(y_train_2018_tensor)
    logging.info(f"D_new (train) ç¸½æ¨£æœ¬æ•¸: {num_new_train_samples} ç­†ã€‚")


    # --- (æ–°) æ­¥é©Ÿ 3.3: Generation Module ç”Ÿæˆ D_old_fake *ä»¥åŒ¹é… D_new_train çš„åˆ†ä½ˆ* ---
    logging.info("--- 3.3: Generation Module æ­£åœ¨ç”Ÿæˆ D_old (fake)... ---")
    try:
        resampled_dfs = []
        
        # è¼‰å…¥ 2017 çš„ LabelEncoder ä»¥ç²å–èˆŠæ¨™ç±¤åˆ—è¡¨
        le_path = os.path.join(artifacts_dir, "label_encoder_2017.joblib")
        le_2017 = joblib.load(le_path)
        scaler_path = os.path.join(artifacts_dir, "minmax_scaler_2017.joblib")
        scaler_2017 = joblib.load(scaler_path)
        
        num_old_labels = len(le_2017.classes_)
        if num_old_labels == 0:
            logging.error("LabelEncoder ä¸­æ²’æœ‰æ¨™ç±¤ã€‚")
            return

        # 1. ğŸ’¡ è¨ˆç®— D_new è¨“ç·´é›† (y_train_2018_tensor) ä¸­å„é¡åˆ¥çš„æ•¸é‡
        # ä½¿ç”¨ torch.bincount å¿«é€Ÿè¨ˆç®—æ¯å€‹ç´¢å¼•ï¼ˆå³é¡åˆ¥ï¼‰çš„æ•¸é‡
        # ç”±æ–¼ y_train_2018_tensor æ˜¯ Long Tensorï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨
        # output: [Count_of_Label_0, Count_of_Label_1, ...]
        
        # ç¢ºä¿ y_train_2018_tensor åœ¨ CPU ä¸Šé€²è¡Œè¨ˆæ•¸
        y_new_train_cpu = y_train_2018_tensor.cpu() 
        
        # ç²å–æ¯å€‹é¡åˆ¥çš„è¨ˆæ•¸
        label_counts_tensor = torch.bincount(y_new_train_cpu)
        
        # å°‡è¨ˆæ•¸è½‰æ›ç‚ºå­—å…¸ï¼Œä»¥ä¾¿æ–¼æŒ‰æ¨™ç±¤ç´¢å¼•æŸ¥æ‰¾ç”Ÿæˆæ•¸é‡
        label_counts = label_counts_tensor.tolist()
        
        # 2. ğŸ’¡ å‹•æ…‹è¨ˆç®—æ¯å€‹èˆŠæ¨™ç±¤è¦ç”Ÿæˆçš„æ•¸é‡ (åŒ¹é… D_new_train çš„æ•¸é‡)
        total_fake_samples = 0
        
        logging.info(f"è³‡æ–™å¹³è¡¡ç­–ç•¥ï¼šåŒ¹é… D_new (train) è¨“ç·´é›† {num_new_train_samples} ç­†çš„åˆ†ä½ˆã€‚")

        # å‘¼å«ç”Ÿæˆå‡½å¼ï¼Œä¸¦æ ¹æ“šç´¢å¼•ç²å–æ•¸é‡
        for idx, label_name in enumerate(le_2017.classes_):
            # æª¢æŸ¥è©²ç´¢å¼•æ˜¯å¦åœ¨è¨ˆæ•¸åˆ—è¡¨ä¸­ï¼Œå¦‚æœä¸åœ¨ (è¡¨ç¤º D_new_train ä¸­æ²’æœ‰æ­¤æ¨™ç±¤)ï¼Œå‰‡ç”Ÿæˆæ•¸é‡ç‚º 0
            if idx < len(label_counts):
                samples_to_generate = label_counts[idx]
            else:
                samples_to_generate = 0
                
            logging.info(f"   -> æ¨™ç±¤ '{label_name}' (Index: {idx})ï¼Œç›®æ¨™ç”Ÿæˆ {samples_to_generate} ç­†ã€‚")

            if samples_to_generate > 0:
                df_old_fake_part = generate_stacked_data(
                    artifacts_dir=artifacts_dir,
                    label=label_name, # é€™è£¡å‡è¨­ generate_stacked_data æ¥å—åŸå§‹æ¨™ç±¤åç¨±
                    num_samples_per_label=samples_to_generate
                )
                resampled_dfs.append(df_old_fake_part)
                total_fake_samples += len(df_old_fake_part)
                
        # 3. å †ç–Šå’Œæ´—ç‰Œ
        if resampled_dfs:
            df_old_fake = pd.concat(resampled_dfs, ignore_index=True)
            # æ´—ç‰Œä»¥æ‰“äº‚ä¸åŒé¡åˆ¥çš„å‡è³‡æ–™
            df_old_fake = df_old_fake.sample(frac=1, random_state=42).reset_index(drop=True)
            logging.info(f"ç¸½å…±æˆåŠŸç”Ÿæˆ {total_fake_samples} ç­† D_old (fake) è³‡æ–™ã€‚")
        else:
            df_old_fake = pd.DataFrame()
            logging.warning("æ²’æœ‰ D_new è¨“ç·´é›†ä¸­çš„æ¨™ç±¤èˆ‡ D_old æ¨™ç±¤åŒ¹é…ï¼Œæœªç”Ÿæˆ D_old (fake) è³‡æ–™ã€‚")
            if df_old_fake is None:
                logging.error("Generation Module æœªèƒ½ç”Ÿæˆè³‡æ–™ã€‚ä¸­æ­¢ã€‚")
                return
        X_old_fake = df_old_fake.drop(columns=['label'])
        y_old_fake = df_old_fake['label']
        X_old_fake = df_old_fake[scaler_2017.feature_names_in_] # ç¢ºä¿æ¬„ä½é †åºæ­£ç¢º
        X_old_fake = scaler_2017.transform(X_old_fake)
        y_old_fake = le_2017.transform(y_old_fake)
        X_old_fake_tensor = torch.tensor(X_old_fake, dtype=torch.float32)
        y_old_fake_tensor = torch.tensor(y_old_fake, dtype=torch.long)
    except Exception as e:
        logging.error(f"Generation Module ç”Ÿæˆå¤±æ•—: {e}", exc_info=True)
        return

    # --- 3.4: åˆä½µ (D_old_fake + D_new_train) ä¸¦è™•ç† ---
    logging.info("--- 3.4: åˆä½µ D_old(fake) å’Œ D_new(train) ... ---")
    X_retrain_tensor = torch.cat([X_old_fake_tensor, X_train_2018_tensor])
    y_retrain_tensor = torch.cat([y_old_fake_tensor, y_train_2018_tensor])
    logging.info(f"å»ºç«‹æ–°çš„æ··åˆè¨“ç·´é›†: {len(X_retrain_tensor)} ç­†è³‡æ–™")
    

    # --- 3.5: é‡æ–°è¨“ç·´ (A-NIDS_model) ---
    logging.info("--- 3.5: è¨“ç·´æ–°çš„ A-NIDS æ¨¡å‹... ---")
    # # å»ºç«‹ä¸€å€‹å…¨æ–°çš„ FCN æ¨¡å‹
    anids_model = detect_module(
        input_features=input_features,
        num_classes=num_classes
    )
    
    # ä½¿ç”¨èˆ‡ Mlp-2017 ç›¸åŒçš„åƒæ•¸é€²è¡Œè¨“ç·´
    anids_model, anids_history = train_model(
        anids_model,
        X_retrain_tensor, y_retrain_tensor,
        X_test_2018_tensor, y_test_2018_tensor, # (æ–°) ä½¿ç”¨ D_new çš„ 30% ä½œç‚ºé©—è­‰é›†
        le_2017,
        args.epochs,
        args.batch_size,
        args.learning_rate
    )
    
    # å„²å­˜ A-NIDS æ¨¡å‹
    model_path = os.path.join(artifacts_dir, "noFCA-NIDS_model.pth")
    torch.save(anids_model.state_dict(), model_path)
    logging.info(f"A-NIDS (æ›´æ–°å¾Œ) æ¨¡å‹å·²å„²å­˜è‡³: {model_path}")
    anids_model.load_state_dict(torch.load(model_path))
    # ç¹ªè£½ A-NIDS è¨“ç·´åœ–
    plot_training_history(anids_history, artifacts_dir, plot_filename="anids_training_history.png")

    # --- 3.6: æœ€çµ‚è©•ä¼° A-NIDS_model ---
    logging.info("="*50)
    logging.info("PHASE 3: A-NIDS æœ€çµ‚è©•ä¼°")
    logging.info("="*50)
    
    # è©•ä¼° 1: æª¢æŸ¥å°ã€Œæ–°è³‡æ–™ã€çš„é©æ‡‰æ€§ (æˆ‘å€‘æœŸæœ›é«˜åˆ†)
    logging.info("--- è©•ä¼° A-NIDS æ¨¡å‹åœ¨ D_new 30% æ¸¬è©¦é›†ä¸Šçš„æ•ˆèƒ½ ---")
    evaluate_model(
        anids_model,
        X_test_2018_tensor, # (æ–°) ä½¿ç”¨ D_new çš„ 30% æ¸¬è©¦é›†
        y_test_2018_tensor, # (æ–°)
        le_2017, # ä»ä½¿ç”¨ 2017 çš„ encoder
        artifacts_dir,
        dataset_name="xss_noFCA" # (æ–°) æª”å
    )
    